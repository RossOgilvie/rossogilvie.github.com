<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <title>An Epistle on the Trinity of Statistics - Ross Ogilvie</title>
  <link rel="stylesheet" type="text/css" href="../../posts/css/default.css" />
  
</head>

<body>
  <div id="header">
    <div id="navigation">
      <a href="http://rossogilvie.id.au">Homepage</a>
      <a href="../../posts">Index</a>
    </div>
  </div>

  <div id="content">
    <h1>An Epistle on the Trinity of Statistics</h1> 
    <div class="info">
    Posted on February  9, 2021
    
        by Ross
    
</div>

<p>An Epistle on the Trinity of Statistics</p>
<p>Dear Sister,</p>
<p>I would divide statistics into three parts.</p>
<h3 id="part-1">– Part 1 –</h3>
<p>The basic level of statistics is summarising data. This is taking averages, percentiles, making graphs, etc. It’s what you see in newspapers.</p>
<h3 id="part-2">– Part 2 –</h3>
<p>Next we have probability theory. This is not quite the same thing as probability in school; it’s the pure maths version. It’s axiomatic, which means you write down a handful of premises and do logical proofs. For example <a href="https://en.wikipedia.org/wiki/Probability_axioms">Kolmogorov axioms</a> and <a href="https://en.wikipedia.org/wiki/Cox%27s_theorem">Cox’s axioms</a> are two sets of axioms. The first set is really abstract: Given a set of things it’s just about assigning a number to each subset. 1. The number assigned to the whole set is 1. 2. If A is a subset of B, then the number for A should be smaller than or equal to the number for B 3. If you have subsets which don’t have common elements, the the number for the union is the sum of the numbers for each subset.</p>
<p>You don’t necessarily have to even interpret them as being about randomness or chance. For example, suppose the set of things is the set of points on a line between 0 and 1. Then giving each subset its “combined length” fits these rules, because the whole set is 1 unit long, taking subsets decreases the length, and the combined length of two non-overlapping subsets is, d’uh, their combined length. Within the conceptual hierarchy of maths, probability theory sits next to length/area/measure theory. In fact, at usyd there’s third year course called measure theory and probability theory is two weeks of it.</p>
<p>Cox’s axioms I think are more interesting. This is not about assigning a number to sets and subsets, but rather about assigning a number to “propositions”. What is a proposition? In philosophical logic, a proposition is a statement that can be true or false. “The sky is blue”, “Socrates has a beard”. And there are philosophical theories about what “true” and “false” means, about categories, about descriptions. In mathematical logic the focus is one the formalism and symbolic relations; you don’t ‘look inside’ the proposition. eg if “A implies B” and “A” are both true, then a deduction is that “B” is true. Or which sets of rules for deduction are equivalent to one another (the system of ‘zeroth order’ is equivalent to the system of tautologies). Cox’s axioms say you should assign a number to every proposition in a way that is compatible with logic operators like “and”, “not”, “implies”, etc. Consider “A: it has a beard” implies “B: its face will be itchy” for instance, then the number for B must be greater than the number for A, because if A is true then B is true, but if A is false than B might be true anyway.</p>
<p>So both are sets of axioms but applied to different things (sets vs propositions). However there are ways of making a correspondence between sets and propositions that make the two approach equivalent, which is why they are both considered the same subject.</p>
<p><strong>Summary thus far:</strong> Descriptive statistics: summarising data Foundations of probability theory: a scheme to turn propositions into numbers, and calculations are compatible with operations on sets and/or logic.</p>
<h3 id="also-part-2">– Also Part 2 –</h3>
<p>Next, let’s pull our head out of the sand. Probability theory is not really about nothing. Just like measure theory is a codification/abstraction of our real-world experience with length, area, and volume, probability theory is based on our experience of chance/random events. It was invented by Pascal because his friend was a chronic gambler and wanted to know which games were best. The idea of probability based on sets works really well for things with finitely-many discrete outcomes, like dice rolls and coin flips. Each outcome gets given a number. Add them up to find the number for compound events (eg {1,3,5}). This is the probability we do in school, with lots of shortcuts to count special scenerios, eg for outcomes which are a sequence of repeated events (roll two dice, etc). In the hierachy of mathematics, these counting tricks are part of combinatorics and not probability theory per se.</p>
<p>Thus far I managed (I hope) to avoid using the word probability for the number assigned to an outcome. That now changes and it’s at this point we have to introduce some assumptions that connect our calculations to the world and an interpretation of those numbers. This is something along the lines of “0 will never occur, 1 will always occur, a bigger number means it is more likely to occur” and “in a roll of a dice, all the options have equal likelihood”. There are a bunch of ‘problems’ here, or atleast thoughts that should be developed more deeply, but they’re philosophical not mathematical problems. When these theories were first developed, the universe was thought to be deterministic. Thereby arises the problem of ‘What does random or likely mean in a deterministic universe?’. Aristotle argues about what it means that something could potentially happen. Or ‘My friend flips a coin and peeks at the answer but doesn’t show me. Is there a even chance it’s heads? Would my friend say the same?’.</p>
<p>But let’s say we are satisfied we have working definitions of random and likely, as every child knows we do for coin flips. Then there are experimental evidence that the prediction is really good. eg 5,067/10,000 from <a href="https://en.wikipedia.org/wiki/John_Edmund_Kerrich">two statisticians</a> stuck in a nazi camp.</p>
<p>At a high level what we have is a model and we are making predictions. The outcomes of the games are a bit boring. More interesting models are things like “the height of australian men follows a gaussian distribution (aka normal distribution, bell curve) with mean 178 cm and standard deviation 7.5 cm”, which allow you to predict stuff like “98% of men will be tall enough to ride the new rollercoaster”. Basically anything you dream could be a model, but experience guides us to a few models that work in many situations; the gaussian distribution and exponential distribution are prime examples. So probability theory usually focuses these common distributions in the same way that geometry focuses triangles and circles.</p>
<p>As models, statistical model are is no different from any other mathematical model. “If I chop a metre stick into two pieces of the same length, each piece will be 50cm” is a prediction based on a mathematical ‘model’ or conceptualisation of length and measurement. Perhaps the most basic mathematical model of the world is counting. It makes predictions like ‘no matter which order you count things, the total will be the same’. And counting is not always a good model: one puddle of water poured on another puddle of water makes one puddle of water. Or the following joke:</p>
<blockquote>
<p>A physicist, a biologist, and a mathematician are sitting in a café and notice people going into and coming out of the house across the street. First they see two people going into the house. Time passes. After a while, they notice three persons coming out of the house. The physicist: “The measurement was not accurate.”<br />
The biologist: “They have reproduced.”<br />
The mathematician: “If one person enters the house, then it will be empty again.”</p>
</blockquote>
<p>You asked me about statistical models and whether they we accurate, as physics models are. This is a false question; they are mathematical models like any other. What is different perhaps is the types of predictions they make, as a physics model might predict a ball to travel 50 m but a statistical model might predict the average the average age of a group to be 27. What do you want to say about the accuracy of these models when reality says the ball when 48 m and the average age was 26 yr 4 mn?</p>
<p>Of course a model might not give just a single number. More sophisticated models include a range of answers. A model may predict that 90% of the time the ball travels between 45 m and 55 m, or that 90% of the time the average age is between 25 and 28. The only way to evaluate this is to do the experiment and compare to the prediction.</p>
<p>When a model turns out to be not accurate, it’s time to go back to the drawing board. Broadly you can two possibilities: tweak or change. Change the model is easy to understand, you alter the model in some way: add a new term to the equation of motion to take account of drag, or change from a gaussian distribution to a log-normal distribution. But statistical model often have free parameters in ways that physics models don’t, so sometimes you think the model is pretty good and you just adjusts the numbers a little bit. Since the class of a statistical model (gaussian distribution vs log-normal etc) is usually chosen on theoretical ground, the former is characterised as a violation of assumptions, whereas the latter is characterised as measurement error (see Part 3.2).</p>
<h3 id="part-3.1">– Part 3.1 –</h3>
<p>So finally, we get to inferential statistics. I think of it as the reverse of predicting from a model: in the previous section we asked the model to say something about the world, now we see what the world has to say about our model.</p>
<p>One subcategory is hypothesis testing. Really, hypothesis is another word for model. Or maybe model is another word for hypothesis. If you took first year statistics, the “P-value” is an example of this. In hypothesis testing you have a model with set parameters, and you predict how likely it would be to see a certain sample. Eg Let’s return to our model about height: heights of australian men with mean 185 cm, sd 7.5 cm. The model predicts that the probability of seeing a sample of 10 men where the mean is 178 cm or lower is 0.2%. (How to calculate this is part of probability theory about the gaussian distribution.) What should we say if we do an experiment and the sample comes out a mean of 178? The naïve argument is clear: what’s more likely, that the model is correct and this is a fluke, or the model is wrong? Clearly the model is wrong.</p>
<p>The most common form of hypothesis testing is null hypothesis testing. This is when you take some binary possibility: a drug works or it doesn’t, the dice is loaded or it isn’t, there is a signal or there is only noise. You then formulate the “null hypothesis”. This is a model where the system is operating according to chance. It’s useful to work with a null hypothesis because you can use one of the well-studied standard models, for example the fair dice model or a model of noise with a gaussian distribution. Note there is still selection of the model here. Then you apply the reasoning from the previous paragraph: what is the likelihood of seeing the result of the test we just performed, if it’s unlikely then the model is wrong. Final step, if the null-hypothesis is wrong, this was a binary possibility so the alternative must be true: the drug works, the dice was loaded, there was a signal.</p>
<p>Of course there are other hypothesis tests than null hypothesis tests. And more sophisticated analysis tries to combine different tests together, to quantify how strongly they support or disconfirm a model (known in this setting as a hypothesis), to take into account broader factors than a single test.</p>
<p>Often the ‘goal’ of a first year stats course is to cover enough material to get to this point. I think this obscures the connections between parts of the theory and what the fundamental concepts and distinctions are.</p>
<h3 id="part-3.2">– Part 3.2 –</h3>
<p>Another large subcategory of inferential statistics trying to adjust your model based on the data, or filling in some unknown values. Eg, you measure the height of 100 men and use their average height as the mean parameter in your model. This is called making a point estimate or ‘fitting’ the model. In this context, this data is called a sample.</p>
<p>Here’s a distinction I haven’t yet commented upon: usually models have parameters (the adjustable numbers) that are the same as their summary statistics. Eg, my model for height had mean and standard deviation as parameters. I guess this is because summary statistics themselves are designed to be good descriptions of a population. But it doesn’t have to be. For example, 1. a parameter in the model might be the maximum allowed value of something. You are unlikely to get the maximum value in a sample, so the best estimate might be the highest observed value plus 1 standard deviation (I just made that up, consult actual stats books if you need that estimate for real). 2. a model of coloured balls in a bag, with a parameter for the number of different colours. If I pull out three balls and get three different colours, how many colours should I guess there are? What if the three balls were all the same colour? 3. A real example you might have come across is “population standard deviation” and “<a href="https://www.khanacademy.org/math/ap-statistics/summarizing-quantitative-data-ap/measuring-spread-quantitative/e/sample-standard-deviation">sample standard deviation</a>”, where there is a difference of the “N” vs “N-1” in the formula. These are poorly named in my opinion. They should be called the “standard deviation” and the “<a href="https://en.wikipedia.org/wiki/Unbiased_estimation_of_standard_deviation">(unbiased) estimate of the standard deviation</a> based on a sample” respectively (and the change is spreading).</p>
<p>Point estimates are often phrased as taking a sample and trying to predict something about the whole population: eg polling vs elections, quality control on a production line. But for the reasons I just explained, I think the proper conception is of a two step process; first estimate the parameter in your model, then use the model to make a prediction. In the case of simple polling, to take an example, you have a model of voters being like a biased coin/die, 60% likely to vote for a certain party. The parameter is that ‘hidden probability’, the estimate is the just taking the same percentage as the sample, and the election outcome prediction is again this same number (a coin with a 60% probability of heads is predicted to land heads 60% of the time).</p>
<p>This might seem tautological, but it’s important to understanding more complicated models. Let’s continue with voter models. A more complicated model might have each voter as a biased coin, whose bias depends on some demographic factors according to some formula. You then have to estimate the numbers in the formula based on the data in your polling sample. With these parameters estimated, you can now apply your model to the population as a whole and make your election prediction.</p>
<p>The advanced topics are thinking about how to do choose how to do the estimate and what properties you want your estimates to have.</p>
<h3 id="also-part-3.2">– Also Part 3.2 –</h3>
<p>Finally finally, and I think this is what you were probably asking about, is confidence intervals of parameters. This is the next level up from a point estimate. Again, you have a sample and a model with unknown parameters. You get your point estimate, huzzah. “But”, you ask yourself, “this was only an estimate, what range of values would it be reasonable to consider based on this data”.</p>
<p>Here we use that fact that in practice most models are just well-developed standard models applied to different situations. As part of probability theory then there are also models of how different point estimates methods behave for each type of standard model. It might sound like models built on models would be more unreliable, but the idea is that these models of models are more like models of coins and dice; they are only based on very simple and well tested assumptions. The second level model says something like, if you did the experiment 100 times, I predict the parameter estimate would be in this certain range 95% of the time.</p>
<p>If you are still sceptical, you can see the testing of these second level models (implicitly) when you read things about the replication crisis. If you look at plots showing p-hacking, the argument is that the p-values should have a certain distribution. Here is an <a href="https://www.discovermagazine.com/mind/reproducibility-crisis-the-plot-thickens">example</a> from the top of googl; you see that the values from replication attempts (where the researchers were begin super-duper careful to do good science and statistics) follow the predicted distribution but the original ones do not.</p>
<p>Like all models though, the model that tells you confidence intervals is based on certain assumptions. Violate those assumptions, and the confidence intervals are not meaningful. The main assumption for the second level model is that the samples are being drawn randomly and independently from the population. This is why (roughly) the second level model is more like dice and coins: it’s picking randomly from a group with equal likelihood. By way of contrast, the height of a person is not really similar to picking a token from a hat. This also gives a nice perspective on why post-hoc subgroup analysis is so bad statistically; it’s like you are looking over the samples and only choose the ones with your desired outcome. This completely violates the randomness of the samples. (Okay, d’uh it’s obviously bad, but we can point out where extactly the chain statistical reasoning is broken)</p>
<h3 id="postscript">– Postscript –</h3>
<p>Oh, I meant to make some point about Frequentism and Bayesianism. So, clearly my explanation of the axioms of probability theory set this up. The set approach is frequentism: the sets are sets of possible outcomes, we have a model where outcomes have a fixed probability, we observe and try to estimate the hidden/underlying probability from observed frequencies.</p>
<p>The proposition approach is Bayesianism: the number is credence (technical term), aka the confidence or level of belief, you should have that a proposition is true. A bayesian model is telling you how confident to be about the ‘outcome propositions’, ie 50% credence that “the flipped coin is heads”.</p>

  </div>

<div id="footer">
    Ross Ogilvie :: rossogilvie.id.au :: ross@rossogilvie.id.au :: Site proudly generated by
    <a href="http://jaspervdj.be/hakyll">Hakyll</a>
</div>
</body>

</html>
